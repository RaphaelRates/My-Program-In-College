 tags: #machine-learning #regression #linear-models #statistics #python

# üìà Regress√£o Linear: Teoria e Implementa√ß√£o

> [!abstract] Vis√£o Geral **Regress√£o Linear** √© um algoritmo de machine learning supervisionado usado para prever valores cont√≠nuos. O objetivo √© encontrar uma rela√ß√£o linear entre as vari√°veis de entrada (features) e a vari√°vel de sa√≠da (target), permitindo fazer previs√µes sobre novos dados.

## üéØ Fundamentos Te√≥ricos

A regress√£o linear parte de uma ideia simples: modelar a rela√ß√£o entre vari√°veis atrav√©s de uma linha reta (ou hiperplano, em m√∫ltiplas dimens√µes). Imagine que voc√™ quer prever o pre√ßo de uma casa baseado em seu tamanho. Intuitivamente, sabemos que casas maiores tendem a ser mais caras, e essa rela√ß√£o pode ser aproximada por uma reta.

### A Equa√ß√£o Fundamental

No caso mais simples, com uma √∫nica vari√°vel de entrada, temos:

```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ
```

Aqui, **y** √© o valor que queremos prever (como o pre√ßo da casa), **x** √© a vari√°vel que conhecemos (o tamanho), **Œ≤‚ÇÄ** √© onde a reta corta o eixo vertical (intercepto), e **Œ≤‚ÇÅ** determina a inclina√ß√£o da reta. O termo **Œµ** representa o erro aleat√≥rio, reconhecendo que nosso modelo nunca ser√° perfeito.

Quando temos m√∫ltiplas vari√°veis de entrada, a equa√ß√£o se expande naturalmente:

```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ
```

Agora, em vez de uma linha em 2D, estamos trabalhando com um hiperplano em m√∫ltiplas dimens√µes. Cada **Œ≤·µ¢** nos diz quanto **y** muda quando **x·µ¢** aumenta em uma unidade, mantendo as outras vari√°veis constantes.

## üìä O M√©todo dos M√≠nimos Quadrados

A grande quest√£o √©: como encontrar os melhores valores para **Œ≤‚ÇÄ** e **Œ≤‚ÇÅ**? O m√©todo dos m√≠nimos quadrados responde isso de forma elegante. A ideia √© minimizar a soma dos quadrados dos erros de previs√£o.

### Entendendo os Res√≠duos

Para cada ponto de dado que temos, podemos calcular a diferen√ßa entre o valor real e o valor previsto pelo nosso modelo. Essa diferen√ßa √© chamada de **res√≠duo**:

```
res√≠duo = y_observado - y_previsto
```

O m√©todo dos m√≠nimos quadrados busca os par√¢metros que minimizam a soma dos quadrados desses res√≠duos. Por que elevar ao quadrado? Primeiro, porque assim erros positivos e negativos n√£o se cancelam. Segundo, porque penaliza mais fortemente erros grandes, o que geralmente √© desej√°vel.

Matematicamente, queremos minimizar:

```
SSE = Œ£(y·µ¢ - ≈∑·µ¢)¬≤ = Œ£(y·µ¢ - Œ≤‚ÇÄ - Œ≤‚ÇÅx·µ¢)¬≤
```

### A Solu√ß√£o Anal√≠tica

A beleza da regress√£o linear √© que existe uma solu√ß√£o fechada, derivada atrav√©s de c√°lculo diferencial. Quando organizamos nossos dados em matrizes, a solu√ß√£o fica:

```
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
```

Esta √© a famosa **Equa√ß√£o Normal**. Aqui, **X** √© nossa matriz de features (com uma coluna de 1s adicionada para o intercepto), **y** √© o vetor de targets, e **Œ≤** cont√©m todos os coeficientes que procuramos. A opera√ß√£o **X·µÄ** significa transposta de X, e o termo **(X·µÄX)‚Åª¬π** √© a inversa da matriz X·µÄX.

A intui√ß√£o por tr√°s dessa f√≥rmula vem do c√°lculo multivariado. Quando derivamos o SSE em rela√ß√£o a cada coeficiente e igualamos a zero (buscando o m√≠nimo), chegamos naturalmente a esta express√£o. √â an√°logo a encontrar o ponto mais baixo de um vale - onde a derivada √© zero.

```python
import numpy as np

def linear_regression_closed_form(X, y):
    """
    Calcula os coeficientes da regress√£o usando a equa√ß√£o normal.
    Esta √© a forma anal√≠tica exata - n√£o requer itera√ß√µes.
    """
    # Adiciona coluna de 1s para o intercepto
    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])
    
    # Aplica a equa√ß√£o normal: Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
    beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y
    
    return beta

# Exemplo pr√°tico
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

coeficientes = linear_regression_closed_form(X, y)
print(f"Intercepto (Œ≤‚ÇÄ): {coeficientes[0]:.2f}")
print(f"Coeficiente (Œ≤‚ÇÅ): {coeficientes[1]:.2f}")
```

## üîÑ Gradiente Descendente: Uma Abordagem Alternativa

Embora a equa√ß√£o normal forne√ßa a solu√ß√£o exata, ela tem limita√ß√µes pr√°ticas. Quando temos milh√µes de features, calcular **(X·µÄX)‚Åª¬π** torna-se computacionalmente invi√°vel (complexidade O(n¬≥)). √â aqui que o gradiente descendente se torna valioso.

### A Ideia Geom√©trica

Imagine que voc√™ est√° em uma montanha enevoada e quer chegar ao vale (o ponto de erro m√≠nimo). Sem enxergar o caminho completo, voc√™ sente a inclina√ß√£o do terreno sob seus p√©s e d√° um passo na dire√ß√£o mais √≠ngreme para baixo. Repete esse processo at√© n√£o conseguir descer mais. Isso √© gradiente descendente.

Matematicamente, come√ßamos com valores aleat√≥rios para os par√¢metros e os atualizamos iterativamente:

```
Œ≤ := Œ≤ - Œ± ¬∑ ‚àáJ(Œ≤)
```

Aqui, **Œ±** √© a taxa de aprendizado (o tamanho do passo), e **‚àáJ(Œ≤)** √© o gradiente da fun√ß√£o de custo (a dire√ß√£o da "ladeira"). O gradiente nos diz como o erro muda quando alteramos cada par√¢metro.

Para a regress√£o linear, os gradientes s√£o:

```
‚àÇJ/‚àÇŒ≤‚ÇÄ = (1/m) ¬∑ Œ£(≈∑·µ¢ - y·µ¢)
‚àÇJ/‚àÇŒ≤‚±º = (1/m) ¬∑ Œ£(≈∑·µ¢ - y·µ¢) ¬∑ x·µ¢‚±º
```

Onde **m** √© o n√∫mero de amostras. Note que o gradiente √© proporcional ao erro e √† feature correspondente.

```python
class LinearRegressionGD:
    def __init__(self, learning_rate=0.01, n_iter=1000):
        self.learning_rate = learning_rate
        self.n_iter = n_iter
        self.coef_ = None
        self.intercept_ = None
        self.loss_history = []
        
    def fit(self, X, y):
        """
        Treina o modelo iterativamente usando gradiente descendente.
        A cada itera√ß√£o, damos um passo na dire√ß√£o que reduz o erro.
        """
        n_samples, n_features = X.shape
        
        # Inicializa par√¢metros com zeros (ponto de partida arbitr√°rio)
        self.coef_ = np.zeros(n_features)
        self.intercept_ = 0
        
        for iteration in range(self.n_iter):
            # Calcula previs√µes com par√¢metros atuais
            y_pred = self.intercept_ + X @ self.coef_
            
            # Calcula o erro
            error = y_pred - y
            
            # Calcula gradientes (dire√ß√£o da "ladeira")
            grad_intercept = (1/n_samples) * np.sum(error)
            grad_coef = (1/n_samples) * (X.T @ error)
            
            # Atualiza par√¢metros dando um passo contr√°rio ao gradiente
            self.intercept_ -= self.learning_rate * grad_intercept
            self.coef_ -= self.learning_rate * grad_coef
            
            # Registra o erro (para monitorar converg√™ncia)
            loss = np.mean(error ** 2)
            self.loss_history.append(loss)
            
            if iteration % 100 == 0:
                print(f"Itera√ß√£o {iteration}, Loss: {loss:.4f}")
        
        return self
    
    def predict(self, X):
        return self.intercept_ + X @ self.coef_

# Exemplo de uso
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

model_gd = LinearRegressionGD(learning_rate=0.01, n_iter=1000)
model_gd.fit(X, y)

print(f"\nIntercepto final: {model_gd.intercept_:.2f}")
print(f"Coeficiente final: {model_gd.coef_[0]:.2f}")
```

## üõ†Ô∏è Implementa√ß√£o Completa

Agora, vamos construir uma classe completa de regress√£o linear que inclui todos os m√©todos necess√°rios para treinamento, previs√£o e avalia√ß√£o:

```python
import numpy as np
import matplotlib.pyplot as plt

class LinearRegression:
    def __init__(self):
        self.coef_ = None
        self.intercept_ = None
        
    def fit(self, X, y):
        """
        Treina o modelo usando a equa√ß√£o normal.
        √â um processo direto: organiza os dados e resolve o sistema linear.
        """
        X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])
        
        # Resolve o sistema linear de uma vez
        coefficients = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y
        
        self.intercept_ = coefficients[0]
        self.coef_ = coefficients[1:]
        
        return self
    
    def predict(self, X):
        """
        Faz previs√µes: simplesmente aplica a equa√ß√£o linear aprendida.
        """
        if self.coef_ is None:
            raise ValueError("Modelo n√£o foi treinado ainda")
            
        return self.intercept_ + X @ self.coef_
    
    def score(self, X, y):
        """
        Calcula o R¬≤ (coeficiente de determina√ß√£o).
        
        R¬≤ mede a propor√ß√£o da vari√¢ncia explicada pelo modelo.
        R¬≤ = 1 significa previs√£o perfeita.
        R¬≤ = 0 significa que o modelo n√£o √© melhor que prever a m√©dia.
        """
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)  # Soma dos quadrados dos res√≠duos
        ss_tot = np.sum((y - np.mean(y)) ** 2)  # Vari√¢ncia total
        r_squared = 1 - (ss_res / ss_tot)
        
        return r_squared

# Demonstra√ß√£o com dados sint√©ticos
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)  # Rela√ß√£o linear verdadeira: y = 4 + 3x + ru√≠do

model = LinearRegression()
model.fit(X, y)

print(f"Coeficientes aprendidos:")
print(f"  Intercepto: {model.intercept_:.2f} (real: 4.0)")
print(f"  Coeficiente: {model.coef_[0]:.2f} (real: 3.0)")
print(f"  R¬≤ Score: {model.score(X, y):.2f}")
```

## üìä Avaliando a Qualidade do Modelo

### M√©tricas de Erro

Diferentes m√©tricas capturam aspectos diferentes do desempenho do modelo. O **MSE** (Mean Squared Error) penaliza fortemente erros grandes devido ao quadrado, enquanto o **MAE** (Mean Absolute Error) trata todos os erros de forma mais uniforme. O **RMSE** (Root Mean Squared Error) √© o MSE na mesma unidade da vari√°vel target, facilitando a interpreta√ß√£o.

```python
class RegressionMetrics:
    @staticmethod
    def mse(y_true, y_pred):
        """
        MSE: m√©dia dos quadrados dos erros.
        Penaliza mais fortemente erros grandes.
        """
        return np.mean((y_true - y_pred) ** 2)
    
    @staticmethod
    def rmse(y_true, y_pred):
        """
        RMSE: raiz do MSE.
        Mesma unidade da vari√°vel target, mais interpret√°vel.
        """
        return np.sqrt(RegressionMetrics.mse(y_true, y_pred))
    
    @staticmethod
    def mae(y_true, y_pred):
        """
        MAE: m√©dia dos valores absolutos dos erros.
        Mais robusto a outliers que MSE.
        """
        return np.mean(np.abs(y_true - y_pred))
    
    @staticmethod
    def r2_score(y_true, y_pred):
        """
        R¬≤: propor√ß√£o da vari√¢ncia explicada.
        Varia de -‚àû a 1, onde 1 √© perfeito.
        """
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

# Avalia√ß√£o completa
y_pred = model.predict(X)

print("\n=== M√©tricas de Avalia√ß√£o ===")
print(f"MSE:  {RegressionMetrics.mse(y, y_pred):.4f}")
print(f"RMSE: {RegressionMetrics.rmse(y, y_pred):.4f}")
print(f"MAE:  {RegressionMetrics.mae(y, y_pred):.4f}")
print(f"R¬≤:   {RegressionMetrics.r2_score(y, y_pred):.4f}")
```

## üìà An√°lise de Res√≠duos

Os res√≠duos (diferen√ßas entre valores reais e previstos) revelam muito sobre a qualidade do modelo. Idealmente, eles devem ser aleat√≥rios, com m√©dia zero e vari√¢ncia constante. Padr√µes nos res√≠duos indicam problemas: se eles formam uma curva, talvez a rela√ß√£o n√£o seja linear; se a dispers√£o aumenta com os valores previstos, temos heterocedasticidade.

```python
def analyze_residuals(X, y, model):
    """
    Analisa res√≠duos para verificar se o modelo √© adequado.
    """
    y_pred = model.predict(X)
    residuals = y - y_pred
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # Res√≠duos vs valores previstos
    # Devemos ver pontos aleat√≥rios em torno de zero
    axes[0].scatter(y_pred, residuals, alpha=0.6)
    axes[0].axhline(y=0, color='red', linestyle='--')
    axes[0].set_xlabel('Valores Previstos')
    axes[0].set_ylabel('Res√≠duos')
    axes[0].set_title('Res√≠duos vs Preditos')
    axes[0].grid(True, alpha=0.3)
    
    # Distribui√ß√£o dos res√≠duos
    # Deve ser aproximadamente normal (sim√©trica, centrada em zero)
    axes[1].hist(residuals, bins=20, alpha=0.7, edgecolor='black')
    axes[1].set_xlabel('Res√≠duos')
    axes[1].set_ylabel('Frequ√™ncia')
    axes[1].set_title('Distribui√ß√£o dos Res√≠duos')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"M√©dia dos res√≠duos: {np.mean(residuals):.4f} (deve estar pr√≥ximo de 0)")
    print(f"Desvio padr√£o: {np.std(residuals):.4f}")

analyze_residuals(X, y, model)
```

## üéØ Pressupostos e Limita√ß√µes

A regress√£o linear faz algumas suposi√ß√µes importantes sobre os dados. Primeiro, assume que existe uma rela√ß√£o linear entre as vari√°veis - se a rela√ß√£o verdadeira √© exponencial ou logar√≠tmica, o modelo ter√° baixo desempenho. Segundo, assume que as observa√ß√µes s√£o independentes umas das outras. Terceiro, requer que a vari√¢ncia dos erros seja constante (homocedasticidade) - erros maiores para valores grandes indicam viola√ß√£o. Quarto, idealmente os res√≠duos seguem uma distribui√ß√£o normal. Por fim, quando h√° m√∫ltiplas features, elas n√£o devem ser altamente correlacionadas (multicolinearidade), pois isso torna os coeficientes inst√°veis.

Verificar esses pressupostos √© crucial. Um modelo que viola essas suposi√ß√µes pode fazer previs√µes ruins ou fornecer infer√™ncias estat√≠sticas incorretas, mesmo que os n√∫meros de avalia√ß√£o pare√ßam bons √† primeira vista.

## üöÄ Exemplo Pr√°tico: Previs√£o de Pre√ßos

Vamos aplicar tudo isso a um exemplo realista - prever pre√ßos de im√≥veis baseado no tamanho:

```python
# Simula dados de pre√ßos de casas
np.random.seed(42)
tamanho_m2 = np.random.normal(150, 50, 100)  # Tamanhos em m¬≤
preco_mil = 50 + 3 * tamanho_m2 + np.random.normal(0, 20, 100)  # Pre√ßo em milhares

X_casas = tamanho_m2.reshape(-1, 1)
y_casas = preco_mil

# Treina modelo
model_casas = LinearRegression()
model_casas.fit(X_casas, y_casas)

print("=== Modelo de Previs√£o de Pre√ßos ===")
print(f"Pre√ßo base: R$ {model_casas.intercept_:.2f} mil")
print(f"Valor por m¬≤: R$ {model_casas.coef_[0]:.2f} mil/m¬≤")
print(f"R¬≤: {model_casas.score(X_casas, y_casas):.2f}")

# Prev√™ pre√ßo de casa de 200m¬≤
nova_casa = np.array([[200]])
preco_previsto = model_casas.predict(nova_casa)
print(f"\nPrevis√£o para casa de 200m¬≤: R$ {preco_previsto[0]:.2f} mil")
```

---

> [!summary] Conclus√£o A regress√£o linear combina simplicidade conceitual com rigor matem√°tico. Sua solu√ß√£o fechada atrav√©s da equa√ß√£o normal fornece resultados exatos e interpret√°veis, enquanto o gradiente descendente oferece escalabilidade para grandes datasets. Compreender profundamente este modelo √© fundamental, pois serve de base para t√©cnicas mais avan√ßadas de machine learning.

## üîó T√≥picos Relacionados

- [[Gradient-Descent-Explained]] - Aprofundamento em otimiza√ß√£o
- [[Regularization-Methods]] - Ridge, Lasso e ElasticNet
- [[Polynomial-Regression]] - Capturando rela√ß√µes n√£o-lineares
- [[Feature-Engineering]] - Prepara√ß√£o de dados para regress√£o