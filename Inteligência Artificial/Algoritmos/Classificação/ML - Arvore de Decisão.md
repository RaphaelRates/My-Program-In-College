tags: #machine-learning #decision-trees #classification #regression #supervised-learning

# üå≥ √Årvores de Decis√£o: Teoria e Implementa√ß√£o

> [!abstract] Vis√£o Geral **√Årvores de Decis√£o** s√£o algoritmos de machine learning supervisionado que tomam decis√µes atrav√©s de uma estrutura hier√°rquica de perguntas. Funcionam tanto para classifica√ß√£o quanto para regress√£o, sendo altamente interpret√°veis e intuitivos - cada previs√£o pode ser explicada atrav√©s do caminho percorrido na √°rvore.

## üéØ A Ideia Fundamental

> [!question] Como Funciona? Imagine que voc√™ quer decidir se deve jogar t√™nis baseado no clima. Voc√™ come√ßa perguntando: "Est√° chovendo?". Se sim, n√£o joga. Se n√£o, pergunta: "Est√° muito quente?". E assim por diante. Cada pergunta divide o problema em casos menores, at√© chegar a uma resposta definitiva.

As √°rvores de decis√£o funcionam exatamente assim. O algoritmo constr√≥i automaticamente uma s√©rie de perguntas (testes em features) que dividem os dados de forma √≥tima. Cada divis√£o busca separar os exemplos de classes diferentes (classifica√ß√£o) ou reduzir a vari√¢ncia dos valores (regress√£o).

> [!tip] Intui√ß√£o Visual
> 
> ```
>                   [Chovendo?]
>                   /         \
>               Sim /           \ N√£o
>                 /               \
>           [N√£o Joga]      [Temp > 30¬∞C?]
>                            /           \
>                        Sim /             \ N√£o
>                          /                 \
>                  [N√£o Joga]              [Joga]
> ```

A beleza est√° na simplicidade: cada decis√£o √© uma pergunta bin√°ria clara. O caminho da raiz at√© a folha conta uma hist√≥ria compreens√≠vel sobre por que aquela previs√£o foi feita.

## üìä Fundamentos Matem√°ticos

> [!important] Conceitos Chave Para construir uma √°rvore √≥tima, precisamos de uma forma matem√°tica de medir "qu√£o boa" √© uma divis√£o. Isso nos leva a tr√™s conceitos fundamentais: entropia, ganho de informa√ß√£o e impureza de Gini.

### Entropia: Medindo a Desordem

A entropia vem da teoria da informa√ß√£o e mede o grau de "surpresa" ou "desordem" em um conjunto de dados. Se todos os exemplos s√£o da mesma classe, n√£o h√° surpresa - a entropia √© zero. Se as classes est√£o igualmente distribu√≠das, a entropia √© m√°xima.

> [!formula] Entropia
> 
> ```
> H(S) = -Œ£ p·µ¢ ¬∑ log‚ÇÇ(p·µ¢)
> 
> Onde:
> - S √© o conjunto de exemplos
> - p·µ¢ √© a propor√ß√£o da classe i
> - A soma percorre todas as classes
> ```

Para um problema bin√°rio com propor√ß√£o p de positivos:

```
H = -p¬∑log‚ÇÇ(p) - (1-p)¬∑log‚ÇÇ(1-p)
```

> [!example] Interpretando Entropia
> 
> - **H = 0**: Todos exemplos s√£o da mesma classe (certeza total)
> - **H = 1**: Classes perfeitamente balanceadas em bin√°rio (m√°xima incerteza)
> - **0 < H < 1**: Alguma predomin√¢ncia de uma classe

```python
import numpy as np

def entropy(y):
    """
    Calcula a entropia de um conjunto de labels.
    Quanto maior a entropia, mais "misturadas" as classes.
    """
    # Conta a frequ√™ncia de cada classe
    _, counts = np.unique(y, return_counts=True)
    
    # Calcula propor√ß√µes
    probabilities = counts / len(y)
    
    # Aplica a f√≥rmula da entropia
    # Evita log(0) filtrando probabilidades zero
    entropy_value = -np.sum([p * np.log2(p) for p in probabilities if p > 0])
    
    return entropy_value

# Exemplo: dataset perfeitamente balanceado
y_balanced = np.array([0, 0, 1, 1])
print(f"Entropia (balanceado): {entropy(y_balanced):.3f}")  # ~1.0

# Exemplo: dataset desbalanceado
y_skewed = np.array([0, 0, 0, 1])
print(f"Entropia (desbalanceado): {entropy(y_skewed):.3f}")  # ~0.81

# Exemplo: dataset puro
y_pure = np.array([0, 0, 0, 0])
print(f"Entropia (puro): {entropy(y_pure):.3f}")  # 0.0
```

### Ganho de Informa√ß√£o

O ganho de informa√ß√£o mede quanto uma divis√£o reduz a entropia. √â a diferen√ßa entre a entropia antes e depois de dividir os dados por uma feature espec√≠fica.

> [!formula] Ganho de Informa√ß√£o
> 
> ```
> IG(S, A) = H(S) - Œ£ (|S·µ•|/|S|) ¬∑ H(S·µ•)
> 
> Onde:
> - S √© o conjunto original
> - A √© o atributo usado para dividir
> - S·µ• s√£o os subconjuntos resultantes
> - |S·µ•|/|S| pondera pela propor√ß√£o de exemplos
> ```

> [!note] Interpreta√ß√£o O ganho de informa√ß√£o responde: "Quanto de incerteza eu removo ao fazer esta pergunta?". Queremos escolher a pergunta que remove o m√°ximo de incerteza poss√≠vel.

```python
def information_gain(X, y, feature_idx, threshold):
    """
    Calcula o ganho de informa√ß√£o ao dividir dados por um threshold.
    
    Este √© o crit√©rio usado pelo algoritmo ID3 cl√°ssico.
    """
    # Entropia antes da divis√£o
    parent_entropy = entropy(y)
    
    # Divide os dados
    left_mask = X[:, feature_idx] <= threshold
    right_mask = ~left_mask
    
    # Se a divis√£o n√£o separa nada, ganho √© zero
    if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:
        return 0
    
    # Entropia ponderada ap√≥s a divis√£o
    n = len(y)
    n_left, n_right = len(y[left_mask]), len(y[right_mask])
    
    child_entropy = (n_left/n) * entropy(y[left_mask]) + \
                    (n_right/n) * entropy(y[right_mask])
    
    # Ganho = redu√ß√£o na entropia
    return parent_entropy - child_entropy
```

### √çndice de Gini

O √≠ndice de Gini √© uma alternativa √† entropia, computacionalmente mais eficiente. Mede a probabilidade de classificar incorretamente um elemento escolhido aleatoriamente.

> [!formula] Impureza de Gini
> 
> ```
> Gini(S) = 1 - Œ£ p·µ¢¬≤
> 
> Onde p·µ¢ √© a propor√ß√£o da classe i
> ```

> [!tip] Compara√ß√£o: Gini vs Entropia
> 
> - **Gini** √© mais r√°pido de calcular (sem logaritmos)
> - **Entropia** √© teoricamente mais fundamentada (teoria da informa√ß√£o)
> - Na pr√°tica, ambos produzem √°rvores muito similares
> - Gini tende a isolar a classe mais frequente, entropia produz √°rvores mais balanceadas

```python
def gini_impurity(y):
    """
    Calcula o √≠ndice de Gini - alternativa mais r√°pida √† entropia.
    """
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    
    # Gini = 1 - soma dos quadrados das probabilidades
    return 1 - np.sum(probabilities ** 2)

# Comparando Gini e Entropia
y_test = np.array([0, 0, 1, 1, 1])
print(f"Gini: {gini_impurity(y_test):.3f}")
print(f"Entropia: {entropy(y_test):.3f}")
```

## üå≤ Construindo a √Årvore

> [!warning] O Algoritmo de Constru√ß√£o A constru√ß√£o da √°rvore √© um processo recursivo chamado **divis√£o recursiva bin√°ria**. Come√ßamos com todos os dados na raiz e repetidamente escolhemos a melhor divis√£o at√© atingir um crit√©rio de parada.

### O Processo Passo a Passo

O algoritmo segue uma estrat√©gia gulosa (greedy): a cada n√≥, escolhe a melhor divis√£o poss√≠vel naquele momento, sem olhar para o futuro. Isso n√£o garante a √°rvore globalmente √≥tima, mas √© computacionalmente vi√°vel.

> [!info] Etapas da Constru√ß√£o
> 
> 1. **Avaliar todas divis√µes poss√≠veis**: Para cada feature, testa todos os valores √∫nicos como threshold
> 2. **Escolher a melhor divis√£o**: Aquela com maior ganho de informa√ß√£o (ou redu√ß√£o de Gini)
> 3. **Dividir os dados**: Cria dois n√≥s filhos com os subconjuntos resultantes
> 4. **Recurs√£o**: Repete o processo para cada filho
> 5. **Parar quando**: Atingir pureza, profundidade m√°xima, ou m√≠nimo de exemplos

```python
class Node:
    """
    Representa um n√≥ na √°rvore de decis√£o.
    Pode ser um n√≥ interno (com pergunta) ou folha (com predi√ß√£o).
    """
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature        # √çndice da feature usada para dividir
        self.threshold = threshold    # Valor de corte
        self.left = left             # Sub√°rvore esquerda (‚â§ threshold)
        self.right = right           # Sub√°rvore direita (> threshold)
        self.value = value           # Valor previsto (apenas em folhas)
    
    def is_leaf(self):
        """Verifica se √© um n√≥ folha (sem filhos)"""
        return self.value is not None

class DecisionTreeClassifier:
    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):
        """
        Par√¢metros de controle da √°rvore:
        - max_depth: Profundidade m√°xima (evita overfitting)
        - min_samples_split: M√≠nimo de exemplos para dividir um n√≥
        - criterion: 'gini' ou 'entropy'
        """
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.criterion = criterion
        self.root = None
    
    def _impurity(self, y):
        """Calcula impureza baseado no crit√©rio escolhido"""
        if self.criterion == 'gini':
            return gini_impurity(y)
        else:
            return entropy(y)
    
    def _best_split(self, X, y):
        """
        Encontra a melhor divis√£o poss√≠vel para este n√≥.
        Testa todas features e todos thresholds.
        """
        best_gain = -1
        best_feature = None
        best_threshold = None
        
        n_features = X.shape[1]
        parent_impurity = self._impurity(y)
        
        # Para cada feature
        for feature_idx in range(n_features):
            # Pega valores √∫nicos como candidatos a threshold
            thresholds = np.unique(X[:, feature_idx])
            
            # Para cada threshold poss√≠vel
            for threshold in thresholds:
                # Divide os dados
                left_mask = X[:, feature_idx] <= threshold
                right_mask = ~left_mask
                
                # Ignora divis√µes que n√£o separam
                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                    continue
                
                # Calcula impureza dos filhos
                n = len(y)
                n_left, n_right = np.sum(left_mask), np.sum(right_mask)
                left_impurity = self._impurity(y[left_mask])
                right_impurity = self._impurity(y[right_mask])
                
                # Impureza ponderada
                child_impurity = (n_left/n) * left_impurity + (n_right/n) * right_impurity
                
                # Ganho de informa√ß√£o
                gain = parent_impurity - child_impurity
                
                # Atualiza melhor divis√£o
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_gain
    
    def _build_tree(self, X, y, depth=0):
        """
        Constr√≥i a √°rvore recursivamente.
        Este √© o cora√ß√£o do algoritmo.
        """
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))
        
        # Crit√©rios de parada
        if (depth >= self.max_depth if self.max_depth else False) or \
           n_samples < self.min_samples_split or \
           n_classes == 1:
            # Cria n√≥ folha com a classe mais comum
            leaf_value = np.argmax(np.bincount(y))
            return Node(value=leaf_value)
        
        # Encontra melhor divis√£o
        best_feature, best_threshold, best_gain = self._best_split(X, y)
        
        # Se n√£o h√° ganho, cria folha
        if best_gain == 0:
            leaf_value = np.argmax(np.bincount(y))
            return Node(value=leaf_value)
        
        # Divide os dados
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        # Constr√≥i sub√°rvores recursivamente
        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        
        # Retorna n√≥ interno
        return Node(
            feature=best_feature,
            threshold=best_threshold,
            left=left_subtree,
            right=right_subtree
        )
    
    def fit(self, X, y):
        """Treina a √°rvore"""
        self.root = self._build_tree(X, y)
        return self
    
    def _predict_sample(self, x, node):
        """Percorre a √°rvore para um √∫nico exemplo"""
        if node.is_leaf():
            return node.value
        
        # Decide qual caminho seguir
        if x[node.feature] <= node.threshold:
            return self._predict_sample(x, node.left)
        else:
            return self._predict_sample(x, node.right)
    
    def predict(self, X):
        """Faz previs√µes para m√∫ltiplos exemplos"""
        return np.array([self._predict_sample(x, self.root) for x in X])
```

## üé® Visualiza√ß√£o e Interpreta√ß√£o

> [!success] A Grande Vantagem Diferente de modelos "caixa-preta", √°rvores de decis√£o s√£o completamente interpret√°veis. Voc√™ pode visualizar exatamente como cada decis√£o √© tomada.

```python
def print_tree(node, feature_names=None, indent=""):
    """
    Imprime a √°rvore em formato texto hier√°rquico.
    Cada n√≠vel de indenta√ß√£o representa um n√≠vel na √°rvore.
    """
    if node.is_leaf():
        print(f"{indent}Predi√ß√£o: Classe {node.value}")
        return
    
    feature_name = f"Feature {node.feature}" if feature_names is None else feature_names[node.feature]
    print(f"{indent}Se {feature_name} <= {node.threshold:.2f}:")
    print_tree(node.left, feature_names, indent + "  ")
    print(f"{indent}Sen√£o:")
    print_tree(node.right, feature_names, indent + "  ")

# Exemplo de uso
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data[:, :2], iris.target  # Usa apenas 2 features para simplicidade

tree = DecisionTreeClassifier(max_depth=3)
tree.fit(X, y)

print("=== Estrutura da √Årvore ===")
print_tree(tree.root, iris.feature_names[:2])
```

## üîß √Årvore de Regress√£o

> [!note] Adapta√ß√£o para Valores Cont√≠nuos Para regress√£o, mudamos apenas dois aspectos: o crit√©rio de divis√£o (usamos vari√¢ncia em vez de impureza) e a predi√ß√£o nas folhas (m√©dia dos valores em vez de classe mais comum).

> [!formula] Crit√©rio para Regress√£o
> 
> ```
> MSE(S) = (1/n) ¬∑ Œ£(y·µ¢ - »≥)¬≤
> 
> Onde »≥ √© a m√©dia dos valores em S
> ```

```python
class DecisionTreeRegressor:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None
    
    def _mse(self, y):
        """Calcula o erro quadr√°tico m√©dio (vari√¢ncia)"""
        if len(y) == 0:
            return 0
        return np.var(y)
    
    def _best_split(self, X, y):
        """Encontra divis√£o que minimiza MSE"""
        best_mse_reduction = -1
        best_feature = None
        best_threshold = None
        
        parent_mse = self._mse(y)
        n_features = X.shape[1]
        
        for feature_idx in range(n_features):
            thresholds = np.unique(X[:, feature_idx])
            
            for threshold in thresholds:
                left_mask = X[:, feature_idx] <= threshold
                right_mask = ~left_mask
                
                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                    continue
                
                # MSE ponderado dos filhos
                n = len(y)
                left_mse = self._mse(y[left_mask])
                right_mse = self._mse(y[right_mask])
                child_mse = (np.sum(left_mask)/n) * left_mse + (np.sum(right_mask)/n) * right_mse
                
                # Redu√ß√£o no MSE
                mse_reduction = parent_mse - child_mse
                
                if mse_reduction > best_mse_reduction:
                    best_mse_reduction = mse_reduction
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_mse_reduction
    
    def _build_tree(self, X, y, depth=0):
        """Constr√≥i √°rvore de regress√£o"""
        n_samples = X.shape[0]
        
        # Crit√©rios de parada
        if (depth >= self.max_depth if self.max_depth else False) or \
           n_samples < self.min_samples_split:
            # Folha: retorna m√©dia dos valores
            return Node(value=np.mean(y))
        
        best_feature, best_threshold, best_reduction = self._best_split(X, y)
        
        if best_reduction == 0:
            return Node(value=np.mean(y))
        
        # Divide e constr√≥i sub√°rvores
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return Node(
            feature=best_feature,
            threshold=best_threshold,
            left=left_subtree,
            right=right_subtree
        )
    
    def fit(self, X, y):
        self.root = self._build_tree(X, y)
        return self
    
    def _predict_sample(self, x, node):
        if node.is_leaf():
            return node.value
        
        if x[node.feature] <= node.threshold:
            return self._predict_sample(x, node.left)
        else:
            return self._predict_sample(x, node.right)
    
    def predict(self, X):
        return np.array([self._predict_sample(x, self.root) for x in X])
```

## ‚ö†Ô∏è Overfitting e Regulariza√ß√£o

> [!danger] O Problema do Overfitting √Årvores de decis√£o t√™m uma tend√™ncia natural ao overfitting. Sem restri√ß√µes, elas crescem at√© memorizar perfeitamente os dados de treino, criando regras extremamente espec√≠ficas que n√£o generalizam.

> [!example] Sintoma de Overfitting Imagine uma √°rvore que cria uma regra como: "Se idade = 27.3 anos E altura = 1.73m E peso = 72.1kg, ent√£o classe A". Isso √© memoriza√ß√£o, n√£o aprendizado.

### T√©cnicas de Poda

A poda controla a complexidade da √°rvore, evitando que ela fique excessivamente detalhada:

> [!tip] Estrat√©gias de Regulariza√ß√£o **Pr√©-poda (Pre-pruning)**:
> 
> - Limita `max_depth` (profundidade m√°xima)
> - Define `min_samples_split` (m√≠nimo de exemplos para dividir)
> - Estabelece `min_samples_leaf` (m√≠nimo de exemplos em folha)
> - Requer `min_impurity_decrease` (ganho m√≠nimo para dividir)
> 
> **P√≥s-poda (Post-pruning)**:
> 
> - Constr√≥i √°rvore completa primeiro
> - Remove sub√°rvores que n√£o melhoram performance em valida√ß√£o
> - Mais custoso computacionalmente, mas teoricamente melhor

```python
# Exemplo: comparando √°rvores com diferentes profundidades
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

depths = [1, 2, 3, 5, 10, None]
print("=== Impacto da Profundidade ===")

for depth in depths:
    tree = DecisionTreeClassifier(max_depth=depth)
    tree.fit(X_train, y_train)
    
    train_acc = accuracy_score(y_train, tree.predict(X_train))
    test_acc = accuracy_score(y_test, tree.predict(X_test))
    
    depth_str = str(depth) if depth else "Ilimitada"
    print(f"Profundidade {depth_str:10s} - Treino: {train_acc:.3f}, Teste: {test_acc:.3f}")
```

## üéØ Vantagens e Limita√ß√µes

> [!success] Vantagens **Interpretabilidade**: Cada previs√£o pode ser explicada por um caminho claro de decis√µes. Perfeito quando √© necess√°rio justificar decis√µes para stakeholders ou requisitos regulat√≥rios.
> 
> **N√£o-param√©trico**: N√£o assume distribui√ß√£o espec√≠fica dos dados. Funciona com rela√ß√µes n√£o-lineares e intera√ß√µes complexas entre features.
> 
> **Features mistas**: Lida naturalmente com features num√©ricas e categ√≥ricas sem necessidade de encoding elaborado.
> 
> **Pouco pr√©-processamento**: N√£o requer normaliza√ß√£o ou padroniza√ß√£o. √â robusto a outliers e valores faltantes.

> [!warning] Limita√ß√µes **Instabilidade**: Pequenas mudan√ßas nos dados podem gerar √°rvores completamente diferentes. Isso prejudica a confiabilidade e reprodutibilidade.
> 
> **Overfitting f√°cil**: Tende a criar modelos muito complexos que memorizam o treino. Requer cuidado com regulariza√ß√£o.
> 
> **Vi√©s em features**: Favorece features com mais valores √∫nicos. Pode ignorar features importantes com poucos valores.
> 
> **Fronteiras de decis√£o limitadas**: S√≥ cria fronteiras paralelas aos eixos (ortogonais). N√£o consegue representar diagonais diretamente, precisando de m√∫ltiplas divis√µes.

## üöÄ Aplica√ß√£o Pr√°tica Completa

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

# Gera dataset sint√©tico
X, y = make_classification(
    n_samples=500,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=42
)

# Treina modelo
tree = DecisionTreeClassifier(max_depth=5, min_samples_split=10)
tree.fit(X, y)

# Valida√ß√£o cruzada
scores = cross_val_score(tree, X, y, cv=5)
print(f"Acur√°cia m√©dia (CV): {scores.mean():.3f} (¬±{scores.std():.3f})")

# Visualiza fronteiras de decis√£o
def plot_decision_boundary(model, X, y):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Fronteiras de Decis√£o da √Årvore')
    plt.show()

plot_decision_boundary(tree, X, y)
```

---

> [!summary] Conclus√£o √Årvores de Decis√£o s√£o algoritmos poderosos e elegantes que combinam simplicidade conceitual com capacidade de modelar rela√ß√µes complexas. Sua interpretabilidade as torna ideais para dom√≠nios onde transpar√™ncia √© crucial. Embora sofram de instabilidade e tend√™ncia ao overfitting, servem de base fundamental para m√©todos ensemble como Random Forest e Gradient Boosting, que superam essas limita√ß√µes mantendo as vantagens.

## üîó T√≥picos Relacionados

- [[ML - Random Florest]] - Ensemble de √°rvores para maior robustez
- [[Gradient-Boosting]] - Combina√ß√£o sequencial de √°rvores