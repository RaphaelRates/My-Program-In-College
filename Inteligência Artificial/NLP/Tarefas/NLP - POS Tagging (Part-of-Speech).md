tags: #nlp #linguistics #pos-tagging #computational-linguistics

# Part-of-Speech Tagging: Da Teoria Ã  ImplementaÃ§Ã£o

> [!abstract] VisÃ£o Geral
> POS Tagging Ã© o processo de **rotular cada palavra em um texto** com sua categoria gramatical correspondente (substantivo, verbo, adjetivo, etc.). Ã‰ uma tarefa fundamental no NLP que serve como base para muitas aplicaÃ§Ãµes mais complexas.

## ğŸ¯ Fundamentos TeÃ³ricos

### O Que SÃ£o Partes do Discurso?

> [!definition] Parte do Discurso (POS)
> Categoria gramatical que classifica palavras baseando-se em:
> - **FunÃ§Ã£o sintÃ¡tica**
> - **Significado lÃ©xico**
> - **Comportamento morfolÃ³gico**

### Categorias Principais

```mermaid
graph TD
    A[Partes do Discurso] --> B[Classes Abertas]
    A --> C[Classes Fechadas]
    
    B --> B1[Substantivos<br/>Nouns]
    B --> B2[Verbos<br/>Verbs]
    B --> B3[Adjetivos<br/>Adjectives]
    B --> B4[AdvÃ©rbios<br/>Adverbs]
    
    C --> C1[PreposiÃ§Ãµes<br/>Prepositions]
    C --> C2[ConjunÃ§Ãµes<br/>Conjunctions]
    C --> C3[Determinantes<br/>Determiners]
    C --> C4[Pronomes<br/>Pronouns]
```

### Tag Sets Populares

> [!example] Penn Treebank Tagset
> | Tag | DescriÃ§Ã£o | Exemplo |
> |-----|------------|---------|
> | NN | Substantivo singular | `cat` |
> | NNS | Substantivo plural | `cats` |
> | VB | Verbo base | `run` |
> | VBD | Verbo passado | `ran` |
> | JJ | Adjetivo | `beautiful` |
> | RB | AdvÃ©rbio | `quickly` |

## ğŸ› ï¸ Abordagens para POS Tagging

### 1. Abordagem Baseada em Regras

```python
# Exemplo simplificado de regras
def rule_based_pos_tag(word):
    rules = {
        'ing$': 'VBG',  # GerÃºndio
        'ed$': 'VBD',   # Passado
        's$': 'NNS',    # Plural
        'ly$': 'RB',    # AdvÃ©rbio
    }
    
    for pattern, tag in rules.items():
        if re.search(pattern, word):
            return tag
    return 'NN'  # PadrÃ£o
```

> [!note] Vantagens
> - Transparente e interpretÃ¡vel
> - Funciona bem para padrÃµes regulares
> - NÃ£o requer dados de treinamento

### 2. Abordagem EstatÃ­stica

> [!theory] Modelo de Markov Oculto (HMM)
> - **Estados ocultos**: Tags POS
> - **ObservaÃ§Ãµes**: Palavras
> - **Probabilidade de transiÃ§Ã£o**: P(tagâ‚‚|tagâ‚)
> - **Probabilidade de emissÃ£o**: P(word|tag)

```python
# PseudocÃ³digo HMM
def hmm_viterbi(sentence, tags):
    # InicializaÃ§Ã£o
    for tag in tags:
        prob[tag] = initial_prob[tag] * emission_prob[wordâ‚|tag]
    
    # RecursÃ£o
    for word in sentence[1:]:
        for tag in tags:
            prob[tag] = max(prev_prob[prev_tag] * 
                           transition_prob[tag|prev_tag] * 
                           emission_prob[word|tag])
    
    return best_sequence
```

### 3. Abordagem com Machine Learning

```python
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import Perceptron

# Features para treinamento
def extract_features(sentence, i):
    word = sentence[i]
    return {
        'word': word,
        'is_capitalized': word[0].isupper(),
        'prefix-1': word[:1],
        'prefix-2': word[:2],
        'prefix-3': word[:3],
        'suffix-1': word[-1:],
        'suffix-2': word[-2:],
        'suffix-3': word[-3:],
        'prev_word': '' if i == 0 else sentence[i-1],
        'next_word': '' if i == len(sentence)-1 else sentence[i+1],
    }
```

## ğŸš€ Abordagem com Deep Learning

### Arquitetura LSTM para POS Tagging

```python
import torch
import torch.nn as nn

class POSTagger(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)
        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)
    
    def forward(self, sentence):
        embeds = self.embedding(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_space, dim=1)
        return tag_scores
```

> [!tip] Transformers Modernos
> Modelos como BERT e RoBERTa alcanÃ§am state-of-the-art usando:
> - **Attention mecanisms**
> - **Contexto bidirecional**
> - **PrÃ©-treinamento em larga escala**

## ğŸ“Š AvaliaÃ§Ã£o de Performance

### MÃ©tricas Principais

> [!info] MÃ©tricas de AvaliaÃ§Ã£o
> ```python
> accuracy = (tags_corretos) / (total_tags)
> precision = TP / (TP + FP)
> recall = TP / (TP + FN)
> f1_score = 2 * (precision * recall) / (precision + recall)
> ```

### Desafios Comuns

> [!warning] Ambiguidade LÃ©xica
> - **Bank**: substantivo (banco financeiro) ou verbo (inclinar)?
> - **Book**: substantivo (livro) ou verbo (reservar)?
> 
> **SoluÃ§Ã£o**: Usar contexto para desambiguizaÃ§Ã£o

> [!warning] Palavras Desconhecidas
> - Neologismos
> - Erros de digitaÃ§Ã£o
> - Termos tÃ©cnicos
> 
> **EstratÃ©gias**: 
> - AnÃ¡lise morfolÃ³gica
> - Fallback para regras
> - Embeddings subword

## ğŸ’¡ AplicaÃ§Ãµes PrÃ¡ticas

### Pipeline de NLP

```mermaid
graph LR
    A[Texto Bruto] --> B[TokenizaÃ§Ã£o]
    B --> C[POS Tagging]
    C --> D[AnÃ¡lise SintÃ¡tica]
    D --> E[ExtraÃ§Ã£o de InformaÃ§Ã£o]
    E --> F[AplicaÃ§Ã£o Final]
```

### Casos de Uso

> [!success] AplicaÃ§Ãµes
> - **AnÃ¡lise sintÃ¡tica** (parsing)
> - **ExtraÃ§Ã£o de informaÃ§Ã£o**
> - **TraduÃ§Ã£o automÃ¡tica**
> - **CorreÃ§Ã£o gramatical**
> - **AnÃ¡lise de sentimentos**
> - **Sistemas de perguntas e respostas**

## ğŸ”§ ImplementaÃ§Ã£o PrÃ¡tica

### Exemplo com NLTK

```python
import nltk
from nltk import word_tokenize, pos_tag

text = "The quick brown fox jumps over the lazy dog"
tokens = word_tokenize(text)
tags = pos_tag(tokens)

print("POS Tags:", tags)
# Output: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), 
#          ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'),
#          ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]
```

### Exemplo com spaCy

```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("The quick brown fox jumps over the lazy dog")

for token in doc:
    print(f"{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}")
```

---

> [!summary] ConclusÃ£o
> - POS Tagging Ã© **fundamental** para pipelines de NLP
> - Evoluiu de **abordagens baseadas em regras** para **modelos neurais profundos**
> - A precisÃ£o moderna alcanÃ§a **~97%** em inglÃªs
> - Continua sendo um **campo ativo de pesquisa** para lÃ­nguas com poucos recursos

## ğŸ“š ReferÃªncias

- [[Jurafsky-Martin-Speech-Language-Processing]]
- [[Manning-Schutze-Statistical-NLP]]
- [[spaCy-Documentation]]
- [[NLTK-Book]]
