
tags: [nlp, text-preprocessing, stemming, linguistics, natural-language-processing]

# üåø Stemming: Redu√ß√£o de Palavras ao Radical

> [!abstract] Defini√ß√£o
> **Stemming** √© o processo de **reduzir palavras flexionadas** √†s suas formas radicais (stems) removendo afixos. √â uma t√©cnica fundamental no pr√©-processamento de texto para NLP.
[]()
## üéØ O Que √© Stemming?

### Conceito Fundamental

> [!definition] Stemming
> Algoritmo heur√≠stico que **corta o final ou in√≠cio das palavras** para obter uma forma comum baseada apenas na **estrutura morfol√≥gica**, n√£o no significado l√©xico.

```mermaid
graph TB
    A[Palavra Flexionada] --> B[Processo de Stemming]
    B --> C[Stem/Radical]
    C --> D[Agrupamento Sem√¢ntico]
```

### Stemming vs Lematiza√ß√£o

> [!example] Compara√ß√£o
> | T√©cnica | Abordagem | Exemplo | Resultado |
> |---------|-----------|---------|-----------|
> | **Stemming** | Heur√≠stica morfol√≥gica | "correndo", "correu", "corredor" | `corr` |
> | **Lematiza√ß√£o** | An√°lise lingu√≠stica | "correndo", "correu", "corredor" | `correr` |

## üìö Algoritmos de Stemming

### 1. Porter Stemmer (Ingl√™s)

#### Regras do Algoritmo Porter

> [!theory] Abordagem por Regras
> O Porter Stemmer aplica **sequ√™ncias de regras** para remover sufixos em etapas:
> - **Step 1a**: SSES ‚Üí SS, IES ‚Üí I, SS ‚Üí SS, S ‚Üí ‚àÖ
> - **Step 1b**: (m>0) EED ‚Üí EE, (*v*) ED ‚Üí ‚àÖ, (*v*) ING ‚Üí ‚àÖ
> - **Step 2**: (m>0) ATIONAL ‚Üí ATE, (m>0) TIONAL ‚Üí TION
> - E assim por diante...

```python
class PorterStemmer:
    def __init__(self):
        self.vowels = 'aeiou'
    
    def is_consonant(self, word, i):
        """Verifica se caractere na posi√ß√£o i √© consoante"""
        if word[i] in self.vowels:
            return False
        if word[i] == 'y' and i > 0 and self.is_consonant(word, i-1):
            return False
        return True
    
    def measure(self, word):
        """Calcula a medida m (n√∫mero de sequ√™ncias VC)"""
        m = 0
        i = 0
        n = len(word)
        
        # Padr√£o: [C]VCVC...[V]
        while i < n:
            if self.is_consonant(word, i):
                # Encontra pr√≥xima vogal
                while i < n and self.is_consonant(word, i):
                    i += 1
                if i < n:
                    m += 1
                    # Encontra pr√≥xima consoante
                    while i < n and not self.is_consonant(word, i):
                        i += 1
            else:
                i += 1
        return m
    
    def step1a(self, word):
        """Step 1a do Porter Stemmer"""
        if word.endswith('sses'):
            return word[:-2]  # ssess -> ss
        elif word.endswith('ies'):
            return word[:-2]  # ies -> i
        elif word.endswith('ss'):
            return word
        elif word.endswith('s'):
            return word[:-1]
        return word

# Exemplo simplificado
stemmer = PorterStemmer()
print(f"measure('trees'): {stemmer.measure('trees')}")
print(f"step1a('cats'): {stemmer.step1a('cats')}")
```

### 2. RSLP Stemmer (Portugu√™s)

#### Especificidades do Portugu√™s

> [!note] RSLP - Removedor de Sufixos da L√≠ngua Portuguesa
> Desenvolvido especificamente para portugu√™s, lida com:
> - **Sufixos nominais**: -mente, -√ß√£o, -mente
> - **Sufixos verbais**: -ando, -endo, -indo
> - **Sufixos plurais**: -s, -es, -√µes

```python
class RSLPStemmer:
    def __init__(self):
        self.suffixes = [
            # Ordem por tamanho (maiores primeiro)
            'abilidades', 'acional', 'ucional', 'amente', 'idades',
            '√¢ncia', '√™ncia', 'mente', 'idade', '√¢ncia', '√™ncia',
            'eza', 'ezas', 'ico', 'ica', 'oso', 'osa',
            'amento', 'imento', 'adora', 'ador', 'a√ß√£o',
            'antes', '√¢ncia', '√°vel', '√≠vel', 'ista', 'ivo',
            'ada', 'ido', 'iva', 'ivo', 'ira', 'eiro',
            '√£o', '√™s', 'ar', 'er', 'ir', 'as', 'es', 'is',
            'a', 'e', 'i', 'o', 'u'
        ]
    
    def stem(self, word):
        """Aplica stemming RSLP simplificado"""
        word_lower = word.lower()
        
        for suffix in self.suffixes:
            if word_lower.endswith(suffix):
                # Verifica se a palavra tem pelo menos 3 caracteres ap√≥s remo√ß√£o
                stemmed = word_lower[:-len(suffix)]
                if len(stemmed) >= 3:
                    return stemmed
        
        return word_lower

# Exemplo
rslp = RSLPStemmer()
palavras = ['correndo', 'felizmente', 'cantando', 'am√°vel', 'computador']
for palavra in palavras:
    stem = rslp.stem(palavra)
    print(f"{palavra:12} ‚Üí {stem}")
```

## üõ†Ô∏è Implementa√ß√µes Pr√°ticas

### Implementa√ß√£o com NLTK

```python
import nltk
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.tokenize import word_tokenize

# Download recursos se necess√°rio
nltk.download('punkt')

# Stemmers dispon√≠veis
porter = PorterStemmer()
snowball_pt = SnowballStemmer('portuguese')

def stem_text_nltk(text, language='portuguese'):
    """
    Aplica stemming em texto usando NLTK
    """
    tokens = word_tokenize(text, language='portuguese')
    
    if language == 'portuguese':
        stemmer = SnowballStemmer('portuguese')
    else:
        stemmer = PorterStemmer()
    
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    
    return ' '.join(stemmed_tokens)

# Exemplos
texto_pt = "Os gatos corriam rapidamente pelo jardim felizmente"
texto_en = "The cats were running quickly through the happy garden"

print("=== Stemming com NLTK ===")
print(f"Portugu√™s: {stem_text_nltk(texto_pt)}")
print(f"Ingl√™s: {stem_text_nltk(texto_en, 'english')}")
```

### Implementa√ß√£o com spaCy + Custom Stemmer

```python
import spacy
import re

class EnhancedPortugueseStemmer:
    def __init__(self):
        # Padr√µes regex para sufixos comuns em portugu√™s
        self.patterns = [
            (r'(amentos|imentos)$', ''),      # substantivos
            (r'(ando|endo|indo)$', ''),       # ger√∫ndio
            (r'(adas|idos|idas)$', ''),       # partic√≠pio
            (r'(ar√°|er√°|ir√°)$', ''),         # futuro
            (r'(ava|ia)$', ''),              # pret√©rito imperfeito
            (r'(√µes|√£es|ais|eis)$', '√£o'),   # plurais
            (r'(mente)$', ''),               # adv√©rbios
            (r'(√≠ssimo|√≠ssima)$', ''),       # superlativo
            (r'(zinhos|zinhas)$', ''),       # diminutivo
            (r'(√≠amo|√≠eis)$', 'ir'),         # verbos
        ]
        
    def stem(self, word):
        original = word.lower()
        
        # Aplica padr√µes na ordem
        for pattern, replacement in self.patterns:
            stemmed = re.sub(pattern, replacement, original)
            if stemmed != original:
                # Verifica se stem tem comprimento m√≠nimo
                if len(stemmed) >= 2:
                    return stemmed
        
        return original

# Uso do stemmer avan√ßado
enhanced_stemmer = EnhancedPortugueseStemmer()
test_words = [
    'correndo', 'felizmente', 'cantar√£o', 'am√°vamos', 
    'gatinhos', 'lind√≠ssima', 'computadores'
]

print("=== Stemming Avan√ßado ===")
for word in test_words:
    stem = enhanced_stemmer.stem(word)
    print(f"{word:15} ‚Üí {stem}")
```

## üìä An√°lise de Resultados

### Compara√ß√£o de Algoritmos

```python
def compare_stemmers(words):
    """
    Compara diferentes algoritmos de stemming
    """
    porter = PorterStemmer()
    snowball_pt = SnowballStemmer('portuguese')
    snowball_es = SnowballStemmer('spanish')
    custom_pt = EnhancedPortugueseStemmer()
    
    print(f"{'Palavra':15} {'Porter':10} {'Snowball PT':12} {'Snowball ES':12} {'Custom PT':12}")
    print("-" * 70)
    
    for word in words:
        results = [
            porter.stem(word),
            snowball_pt.stem(word),
            snowball_es.stem(word),
            custom_pt.stem(word)
        ]
        print(f"{word:15} {results[0]:10} {results[1]:12} {results[2]:12} {results[3]:12}")

# Teste comparativo
test_words = ['running', 'correndo', 'happiness', 'felicidade', 'computadores']
compare_stemmers(test_words)
```

### Visualiza√ß√£o do Processo

```python
import matplotlib.pyplot as plt

def visualize_stemming_examples():
    """
    Visualiza exemplos de stemming
    """
    stemmer_pt = SnowballStemmer('portuguese')
    
    examples = {
        'Verbos': ['correr', 'correndo', 'correu', 'correr√£o'],
        'Substantivos': ['casa', 'casas', 'casinha', 'casebre'],
        'Adjetivos': ['feliz', 'felizmente', 'felicidade', 'infeliz'],
        'Adv√©rbios': ['r√°pido', 'rapidamente', 'r√°pidos', 'rapidez']
    }
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()
    
    for idx, (category, words) in enumerate(examples.items()):
        stems = [stemmer_pt.stem(word) for word in words]
        
        # Plot
        x_pos = range(len(words))
        axes[idx].bar(x_pos, [1] * len(words), alpha=0.7, label='Original')
        axes[idx].set_xticks(x_pos)
        axes[idx].set_xticklabels(words, rotation=45)
        axes[idx].set_title(f'{category} - Stemming')
        axes[idx].grid(True, alpha=0.3)
        
        # Adiciona stems como texto
        for i, (word, stem) in enumerate(zip(words, stems)):
            axes[idx].text(i, 0.5, stem, ha='center', va='center', 
                          fontweight='bold', color='red')
    
    plt.tight_layout()
    plt.show()

visualize_stemming_examples()
```

## üéØ Aplica√ß√µes Pr√°ticas

### Sistema de Busca com Stemming

```python
class SearchEngineWithStemming:
    def __init__(self, documents):
        self.documents = documents
        self.stemmer = SnowballStemmer('portuguese')
        self.stemmed_index = self._build_index()
    
    def _build_index(self):
        """
        Constr√≥i √≠ndice invertido com stems
        """
        index = {}
        for doc_id, document in enumerate(self.documents):
            tokens = word_tokenize(document.lower())
            stems = [self.stemmer.stem(token) for token in tokens 
                    if token.isalpha()]  # Remove pontua√ß√£o
            
            for stem in set(stems):  # Usa set para evitar duplicatas
                if stem not in index:
                    index[stem] = []
                index[stem].append(doc_id)
        
        return index
    
    def search(self, query):
        """
        Busca documentos relevantes usando stemming
        """
        query_tokens = word_tokenize(query.lower())
        query_stems = [self.stemmer.stem(token) for token in query_tokens 
                      if token.isalpha()]
        
        # Encontra documentos relevantes
        relevant_docs = set()
        for stem in query_stems:
            if stem in self.stemmed_index:
                relevant_docs.update(self.stemmed_index[stem])
        
        return [self.documents[doc_id] for doc_id in relevant_docs]

# Exemplo de uso
documents = [
    "Os gatos est√£o correndo no jardim",
    "O cachorro corre rapidamente",
    "Gatos e cachorros s√£o animais dom√©sticos",
    "O jardim tem flores bonitas"
]

engine = SearchEngineWithStemming(documents)
results = engine.search("gato correndo")

print("=== Resultados da Busca ===")
for result in results:
    print(f"- {result}")
```

### Pr√©-processamento para Machine Learning

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

class StemmingTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, language='portuguese'):
        self.language = language
        self.stemmer = SnowballStemmer(language)
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        """
        Aplica stemming em uma lista de textos
        """
        stemmed_texts = []
        for text in X:
            tokens = word_tokenize(text.lower())
            stems = [self.stemmer.stem(token) for token in tokens 
                    if token.isalpha()]
            stemmed_texts.append(' '.join(stems))
        
        return stemmed_texts

# Pipeline completo com stemming
def create_stemming_pipeline():
    return Pipeline([
        ('stemming', StemmingTransformer()),
        ('tfidf', TfidfVectorizer(max_features=1000)),
    ])

# Exemplo
texts = [
    "Estou aprendendo processamento de linguagem natural",
    "O processamento de texto √© importante para NLP",
    "Stemming √© uma t√©cnica de pr√©-processamento",
    "Reduzir palavras aos seus radicais melhora resultados"
]

pipeline = create_stemming_pipeline()
X_transformed = pipeline.fit_transform(texts)

print("Dimens√µes da matriz TF-IDF:", X_transformed.shape)
print("Feature names:", pipeline.named_steps['tfidf'].get_feature_names_out()[:10])
```

## üìà Avalia√ß√£o de Desempenho

### M√©tricas de Qualidade

```python
def evaluate_stemmer_quality(stemmer, word_groups):
    """
    Avalia a qualidade do stemmer usando grupos de palavras relacionadas
    """
    total_groups = len(word_groups)
    correct_groups = 0
    
    for group in word_groups:
        stems = [stemmer.stem(word) for word in group]
        # Considera correto se todas as palavras do grupo t√™m o mesmo stem
        if len(set(stems)) == 1:
            correct_groups += 1
    
    accuracy = correct_groups / total_groups
    return accuracy

# Grupos de teste
test_groups = [
    ['correr', 'correndo', 'correu', 'correr√°'],
    ['feliz', 'felizmente', 'felicidade'],
    ['casa', 'casas', 'casinha'],
    ['computador', 'computadores', 'computa√ß√£o']
]

stemmer_pt = SnowballStemmer('portuguese')
accuracy = evaluate_stemmer_quality(stemmer_pt, test_groups)

print(f"Acur√°cia do Stemmer: {accuracy:.1%}")
```

### An√°lise de Overstemming e Understemming

> [!important] Problemas Comuns
> 
> **Overstemming**: Palavras diferentes reduzidas ao mesmo stem
> ```python
> # Exemplo: "universal" e "universidade" ‚Üí "univers"
> overstemming_examples = [
>     ('universal', 'universidade'),
>     ('argumento', 'argumenta√ß√£o'),
>     ('real', 'realidade')
> ]
> ```
> 
> **Understemming**: Palavras relacionadas n√£o reduzidas ao mesmo stem
> ```python
> # Exemplo: "correr" e "corrida" ‚Üí stems diferentes
> understemming_examples = [
>     ('correr', 'corrida'),
>     ('cantar', 'canto'),
>     ('andar', 'andamento')
> ]
> ```

```python
def analyze_stemming_errors(stemmer, word_pairs):
    """
    Analisa overstemming e understemming
    """
    overstemming_count = 0
    understemming_count = 0
    
    for word1, word2 in word_pairs:
        stem1 = stemmer.stem(word1)
        stem2 = stemmer.stem(word2)
        
        # S√£o palavras semanticamente relacionadas?
        are_related = True  # Em pr√°tica, usar dicion√°rio sem√¢ntico
        
        if stem1 == stem2 and not are_related:
            overstemming_count += 1
            print(f"OVERSTEMMING: '{word1}' ‚Üí '{stem1}', '{word2}' ‚Üí '{stem2}'")
        elif stem1 != stem2 and are_related:
            understemming_count += 1
            print(f"UNDERSTEMMING: '{word1}' ‚Üí '{stem1}', '{word2}' ‚Üí '{stem2}'")
    
    print(f"\nOverstemming: {overstemming_count}")
    print(f"Understemming: {understemming_count}")

# Teste
test_pairs = [('universal', 'universidade'), ('correr', 'corrida')]
analyze_stemming_errors(stemmer_pt, test_pairs)
```

## üöÄ Melhores Pr√°ticas

### Quando Usar Stemming

> [!success] Casos de Uso Ideais
> - **Sistemas de busca** e information retrieval
> - **An√°lise de texto** para minera√ß√£o de opini√£o
> - **Classifica√ß√£o de documentos**
> - **Detec√ß√£o de pl√°gio**
> - **An√°lise de similaridade** textual

### Quando Evitar Stemming

> [!warning] Limita√ß√µes
> - **Aplica√ß√µes que precisam de significado preciso**
> - **Processamento sint√°tico** complexo
> - **Lematiza√ß√£o** √© prefer√≠vel quando dispon√≠vel
> - **L√≠nguas com morfologia complexa**

### Pipeline Recomendado

```mermaid
graph TD
    A[Texto Bruto] --> B[Limpeza e Normaliza√ß√£o]
    B --> C[Tokeniza√ß√£o]
    C --> D[Remo√ß√£o de Stop Words]
    D --> E{Precisa de Stemming?}
    E -->|Sim| F[Aplica Stemming]
    E -->|N√£o| G[Mant√©m Tokens Originais]
    F --> H[Processamento Posterior]
    G --> H
```

---

> [!summary] Conclus√£o
> **Stemming** √© uma t√©cnica poderosa mas com limita√ß√µes:
> - **Reduz dimensionalidade** e **melhora recall**
> - **Heur√≠stico** ‚Üí pode cometer erros
> - **Espec√≠fico por idioma** ‚Üí escolha o algoritmo correto
> - **Complementar** a outras t√©cnicas de NLP
