# ImplementaÃ§Ã£o de Rede Neural com Embedding para ClassificaÃ§Ã£o de Spam

## ğŸ“‹ VisÃ£o Geral do Processo

Neste projeto, adaptamos uma rede neural para classificaÃ§Ã£o de mensagens como "spam" ou "nÃ£o spam" (ham), utilizando uma abordagem mais sofisticada com camadas de embedding em vez da vetorizaÃ§Ã£o tradicional CountVectorizer.

## ğŸ”„ AdaptaÃ§Ã£o do Notebook Original

### CriaÃ§Ã£o da CÃ³pia do Ambiente
```python
# Arquivo â†’ Salvar uma cÃ³pia no Drive
# Nome: "Neural_dois_Neuro"
```
**Objetivo:** Criar um ambiente isolado para experimentaÃ§Ã£o sem afetar o cÃ³digo original. O nome "Neural_dois" indica que esta Ã© uma segunda versÃ£o neural melhorada.

## ğŸ§© ModificaÃ§Ãµes nas ImportaÃ§Ãµes

### Bibliotecas Adicionadas
```python
from keras.layers import Embedding, Flatten
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

### ğŸ” AnÃ¡lise das Novas ImportaÃ§Ãµes

#### **Embedding Layer**
- **FunÃ§Ã£o:** Cria representaÃ§Ãµes vetoriais densas das palavras
- **Vantagem:** Aprende embeddings especÃ­ficos para o domÃ­nio durante o treinamento
- **ComparaÃ§Ã£o:** Substitui o CountVectorizer que criava representaÃ§Ãµes esparsas

#### **Flatten Layer**
- **FunÃ§Ã£o:** "Achata" a saÃ­da multidimensional do embedding para conectar com camadas densas
- **Necessidade:** Converte formato 2D+ para 1D, exigido pelas camadas Dense

#### **Tokenizer**
- **Substitui:** CountVectorizer do approach anterior
- **Funcionalidade:** Converte texto em sequÃªncias numÃ©ricas (tokens)

#### **pad_sequences**
- **Problema resolvido:** Mensagens tÃªm comprimentos variÃ¡veis
- **SoluÃ§Ã£o:** Padroniza o tamanho das sequÃªncias para alimentar a rede neural

## ğŸ“Š Upload e PreparaÃ§Ã£o dos Dados

### ReinicializaÃ§Ã£o do Ambiente
```python
# Editar â†’ Limpar todas as saÃ­das
# Upload do arquivo spam.csv novamente
```
**Motivo:** Cada sessÃ£o do Colab Ã© isolada, necessitando reupload dos dados.

### Processo de Label Encoding
```python
labelEncoder = LabelEncoder()
y = labelEncoder.fit_transform(spam['Category'])
```
**TransformaÃ§Ã£o:** Converte labels textuais ("spam", "ham") para numÃ©ricos (1, 0)

## ğŸ”¤ TokenizaÃ§Ã£o do Texto

### ConfiguraÃ§Ã£o do Tokenizer
```python
token = Tokenizer(num_words=1000)
```

#### **ParÃ¢metro num_words=1000**
- **FunÃ§Ã£o:** Define o vocabulÃ¡rio mÃ¡ximo como 1000 palavras
- **SeleÃ§Ã£o:** MantÃ©m as 1000 palavras mais frequentes
- **Descarte:** Palavras menos frequentes sÃ£o ignoradas

#### **Processo de Fit e Transform**
```python
token.fit_on_texts(X_train)
X_train = token.texts_to_sequences(X_train)
X_test = token.texts_to_sequences(X_test)
```

### ğŸ” MÃ©todos do Tokenizer

#### **fit_on_texts()**
- **FunÃ§Ã£o:** Aprende o vocabulÃ¡rio a partir dos textos de treino
- **Processo interno:** Cria um mapeamento palavra â†’ Ã­ndice

#### **texts_to_sequences()**
- **FunÃ§Ã£o:** Converte textos em sequÃªncias numÃ©ricas
- **Exemplo:** "Hello world" â†’ [12, 45] (onde 12="hello", 45="world")

## ğŸ“ PadronizaÃ§Ã£o de SequÃªncias

### AplicaÃ§Ã£o do pad_sequences
```python
X_train = pad_sequences(X_train, maxlen=500, padding='post')
X_test = pad_sequences(X_test, maxlen=500, padding='post')
```

### ğŸ“ ParÃ¢metros Detalhados

#### **maxlen=500**
- **FunÃ§Ã£o:** Define comprimento mÃ¡ximo de 500 tokens por mensagem
- **Truncamento:** Mensagens >500 tokens sÃ£o cortadas
- **Preenchimento:** Mensagens <500 tokens recebem padding

#### **padding='post'**
- **Comportamento:** Adiciona zeros no final da sequÃªncia
- **Alternativa:** `padding='pre'` (pad no inÃ­cio - padrÃ£o)
- **Vantagem do 'post':** Preserva o inÃ­cio da mensagem que geralmente contÃ©m informaÃ§Ã£o mais relevante

### ğŸ¯ PropÃ³sito do Padding
```python
# Antes do padding: sequÃªncias de tamanhos variÃ¡veis
[ [1, 45, 23, 67], 
  [12, 8], 
  [45, 23, 67, 89, 12, 34, 56] ]

# Depois do padding: matriz uniforme
[ [1, 45, 23, 67, 0, 0, 0], 
  [12, 8, 0, 0, 0, 0, 0], 
  [45, 23, 67, 89, 12, 34, 56] ]
```

## ğŸ—ï¸ ConstruÃ§Ã£o da Arquitetura Neural

### Camada de Embedding
```python
modelo.add(Embedding(
    input_dim=len(token.word_index) + 1,
    output_dim=50,
    input_length=500
))
```

#### **input_dim**
- **CÃ¡lculo:** `len(token.word_index) + 1`
- **FunÃ§Ã£o:** Tamanho do vocabulÃ¡rio + 1 (para o Ã­ndice 0 usado no padding)
- **ExplicaÃ§Ã£o:** Define quantas palavras diferentes a camada precisa mapear

#### **output_dim=50**
- **Significado:** Tamanho do vetor de embedding para cada palavra
- **InterpretaÃ§Ã£o:** Cada palavra Ã© representada por um vetor de 50 dimensÃµes
- **Vantagem:** Captura relaÃ§Ãµes semÃ¢nticas entre palavras

#### **input_length=500**
- **CorrespondÃªncia:** Mesmo valor do `maxlen` no pad_sequences
- **FunÃ§Ã£o:** Informa o comprimento fixo das sequÃªncias de entrada

### ğŸ”„ Processo do Embedding

#### **TransformaÃ§Ã£o Realizada:**
```
Entrada: [12, 45, 23, 0, 0, ...]  # 500 elementos
â†“ Embedding Layer
SaÃ­da: [
    [0.1, 0.4, -0.2, ..., 0.8],  # vetor para palavra 12
    [0.3, -0.1, 0.9, ..., -0.4], # vetor para palavra 45  
    [0.7, 0.2, -0.5, ..., 0.1],  # vetor para palavra 23
    [0.0, 0.0, 0.0, ..., 0.0],   # vetor para padding
    ...
]  # Formato: (500, 50)
```

### Camada Flatten
```python
modelo.add(Flatten())
```

#### **Necessidade:**
- **Problema:** SaÃ­da do Embedding Ã© 2D (500, 50)
- **SoluÃ§Ã£o:** Flatten transforma em 1D (500 * 50 = 25000 elementos)
- **PropÃ³sito:** Compatibilidade com camadas Dense seguintes

### Camadas Dense e Dropout
```python
modelo.add(Dense(128, activation='relu'))
modelo.add(Dropout(0.3))
modelo.add(Dense(1, activation='sigmoid'))
```

#### **Arquitetura Final:**
1. **Embedding:** (500, 50) â†’ Aprende representaÃ§Ãµes de palavras
2. **Flatten:** (500, 50) â†’ (25000) â†’ Prepara para camadas densas
3. **Dense(128):** Processamento nÃ£o-linear
4. **Dropout(0.3):** RegularizaÃ§Ã£o (evita overfitting)
5. **Dense(1):** ClassificaÃ§Ã£o binÃ¡ria final

## ğŸ¯ CompilaÃ§Ã£o do Modelo

```python
modelo.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
```

### **ConfiguraÃ§Ã£o Mantida:**
- Mesmos parÃ¢metros do approach anterior para comparaÃ§Ã£o justa

## ğŸ‹ï¸ Processo de Treinamento

### HiperparÃ¢metros
```python
modelo.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_test, y_test)
)
```

### ğŸ“Š AnÃ¡lise do Treinamento

#### **epochs=20**
- **Significado:** 20 passadas completas pelo dataset de treino
- **Monitoramento:** Acompanhamento da loss e accuracy em tempo real

#### **batch_size=32**
- **Processamento:** 32 amostras por vez antes de atualizar pesos
- **Vantagem:** Balance entre eficiÃªncia e estabilidade

#### **Dados de ValidaÃ§Ã£o**
- **Uso:** `validation_data=(X_test, y_test)`
- **PropÃ³sito:** Monitorar performance em dados nÃ£o vistos durante treino

## ğŸ“ˆ Resultados e Performance

### MÃ©tricas Finais
- **Loss final:** ~0.09
- **AcurÃ¡cia:** ~0.98
- **ComparaÃ§Ã£o com approach anterior:** Performance similar

### Matriz de ConfusÃ£o
```
[ [Verdadeiros Negativos, Falsos Positivos],
  [Falsos Negativos, Verdadeiros Positivos] ]
```

**Resultado Observado:**
- 19 erros de classificaÃ§Ã£o
- Performance consistente entre treino e validaÃ§Ã£o

## ğŸ’¡ Insights e ConsideraÃ§Ãµes

### Vantagens do Approach com Embedding
1. **Aprendizado EspecÃ­fico:** Embeddings adaptados ao domÃ­nio de spam
2. **RepresentaÃ§Ã£o Rica:** Vetores densos capturam nuances semÃ¢nticas
3. **Escalabilidade:** Melhor para vocabulÃ¡rios grandes

### ComparaÃ§Ã£o com Approach Anterior
- **Performance:** Similar nos dois mÃ©todos
- **Complexidade:** Embedding Ã© mais sofisticado
- **Aplicabilidade:** Embedding Ã© preferÃ­vel para textos complexos

### âš ï¸ ObservaÃ§Ãµes Importantes
- **NÃ£o hÃ¡ diferenÃ§a significativa** de performance entre os mÃ©todos para este dataset especÃ­fico
- A escolha do mÃ©todo depende do contexto e complexidade do problema
- O embedding oferece mais flexibilidade para problemas de NLP mais complexos

## ğŸ¯ ConclusÃ£o

A implementaÃ§Ã£o com camadas de embedding demonstra uma abordagem mais moderna e poderosa para processamento de linguagem natural, embora para este caso especÃ­fico de detecÃ§Ã£o de spam os resultados tenham sido equivalentes ao mÃ©todo tradicional. A arquitetura desenvolvida serve como base para problemas de NLP mais complexos.