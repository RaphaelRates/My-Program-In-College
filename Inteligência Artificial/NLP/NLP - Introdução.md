> [!info] ## ğŸ’¬ O que Ã© NLP
> **NLP** (*Natural Language Processing* â€” Processamento de Linguagem Natural) Ã© um ramo da **InteligÃªncia Artificial** que permite que computadores **compreendam**, **interpretem** e **gerem** linguagem humana â€” seja em portuguÃªs, inglÃªs, japonÃªs ou qualquer outra.
>
> Em resumo, sistemas de NLP sÃ£o capazes de:
> - ğŸ§  **Entender** texto (ex.: anÃ¡lise de sentimento, classificaÃ§Ã£o de intenÃ§Ãµes)
> - âœï¸ **Gerar** texto (ex.: chatbots, resumos, traduÃ§Ãµes)
> - ğŸ” **Extrair** informaÃ§Ã£o (ex.: reconhecimento de entidades, busca semÃ¢ntica)

> [!tip] ## ğŸ§© Pipeline de NLP
> ![[Pasted image 20251009112304.png]]
>
> ### âš™ï¸ Etapa 1 â€” IngestÃ£o de Dados
> A **ingestÃ£o** Ã© o ponto de partida de qualquer pipeline de NLP.  
> Aqui, os dados brutos sÃ£o **coletados**, **centralizados** e **preparados** para processamento posterior.
>
> ğŸ”— **Fontes comuns de dados:**
> - ğŸ—„ï¸ **Bancos de dados** â€” registros estruturados (ex.: SQL, NoSQL)
> - ğŸ“„ **Documentos digitais** â€” PDFs, textos, planilhas, relatÃ³rios
> - â˜ï¸ **ServiÃ§os em nuvem** â€” buckets (S3, GCS), APIs, logs
> - ğŸ§¾ **OCR (Reconhecimento Ã“ptico de Caracteres)** â€” digitaliza texto a partir de imagens ou scans
> - ğŸŒ **Web Scraping** â€” coleta automÃ¡tica de conteÃºdo na web
>
> > [!attention] ğŸ’¡ *Dica:* Nessa fase, o foco Ã© **obter dados com qualidade** e **garantir padronizaÃ§Ã£o** (codificaÃ§Ã£o, idioma, formato).  
> > Dados sujos aqui = modelo ruim lÃ¡ na frente.
> 
> ---
> 
>  ### ğŸ§¼ 2. PrÃ©-processamento
> Ã‰ aqui que o texto cru passa por uma faxina cirÃºrgica pra virar dado Ãºtil.  
> O objetivo Ã© **eliminar ruÃ­dos**, **padronizar formatos** e **destacar o que realmente importa**.
>
> ğŸ”§ **Principais etapas:**
> - ğŸ§¹ RemoÃ§Ã£o de **pontuaÃ§Ã£o**, **[[NLP - Stop Words]]** e **caracteres inÃºteis**
> - ğŸ”¡ **NormalizaÃ§Ã£o** de texto (minÃºsculas, acentuaÃ§Ã£o, emojis, etc.)
> - âœ‚ï¸ **TokenizaÃ§Ã£o** â€” quebra o texto em palavras ou sentenÃ§as
> - ğŸ§¬ **LematizaÃ§Ã£o / [[NLP - Stemming]]** â€” reduz palavras Ã  forma base
>
> ğŸ§  **Conceitos-chave aplicados nessa fase:**
> - ğŸ·ï¸ **[[NLP - POS Tagging (Part-of-Speech)]]:** identifica a funÃ§Ã£o de cada palavra (substantivo, verbo, adjetivo...)  
> - ğŸª¶ **[[NLP - Annotation]]:** rotula partes do texto com informaÃ§Ãµes semÃ¢nticas, sintÃ¡ticas ou contextuais  
> - âš™ï¸ **Engenharia de Atributos:** cria novas features a partir do texto (ex.: contagem de palavras, polaridade, frequÃªncia)
>
> > [!attention] ğŸ’¡ *Dica:* PrÃ©-processar Ã© como afiar a lÃ¢mina antes de cortar â€” se pular essa parte, o modelo sÃ³ vai mastigar o texto sem entender
> ---
> ### ğŸ§± 3. RepresentaÃ§Ã£o Textual - Da Linguagem para a MatemÃ¡tica
>
> #### ğŸ¯ **A Ponte entre Humanos e MÃ¡quinas**
> Transformamos palavras em **vetores numÃ©ricos** que preservam relaÃ§Ãµes semÃ¢nticas e sintÃ¡ticas.
>
> #### ğŸ“Š **EvoluÃ§Ã£o das TÃ©cnicas de Embedding**
> 
### **1. ğŸª MÃ©todos EstatÃ­sticos ClÃ¡ssicos**
| TÃ©cnica | Mecanismo | Casos de Uso |
|---------|-----------|-------------|
| **Bag of Words (BoW)** | Matriz de contagem de termos | Baseline rÃ¡pido para classificaÃ§Ã£o |
| **TF-IDF** | PonderaÃ§Ã£o por relevÃ¢ncia (frequÃªncia inversa) | Sistemas de recomendaÃ§Ã£o, search |
| **One-Hot Encoding** | Vetores esparsos binÃ¡rios | PrÃ©-processamento para redes neurais |

**ğŸ’¡ Insight:** `"rei" - "homem" + "mulher" = "rainha"` â†’ **NÃƒO FUNCIONA** com esses mÃ©todos

### **2. ğŸ§  Word Embeddings EstÃ¡ticos**
| Modelo | Arquitetura | Vantagem |
|--------|-------------|----------|
| **Word2Vec** | Skip-gram/CBOW | Captura relaÃ§Ãµes semÃ¢nticas |
| **GloVe** | FatorizaÃ§Ã£o de matriz co-ocorrÃªncia | Combina estatÃ­stica + aprendizado |
| **FastText** | N-gramas de caracteres | Lida com palavras desconhecidas |

**ğŸš€ Breakthrough:** `vetor("rei") - vetor("homem") + vetor("mulher") â‰ˆ vetor("rainha")`

### **3. ğŸ­ Embeddings Contextuais (RevoluÃ§Ã£o)**
| Modelo | InovaÃ§Ã£o | Impacto |
|--------|----------|---------|
| **BERT** | Bidirecional + Masked LM | Entende contexto completo |
| **GPT** | Decoder-only autoregressivo | GeraÃ§Ã£o de texto coerente |
| **T5** | Text-to-Text unified framework | Transforma qualquer tarefa em texto |

**ğŸ¯ DiferenÃ§a Crucial:** 
- **Static:** "banco" = mesmo vetor em todos contextos
- **Contextual:** "banco financeiro" â‰  "banco de praÃ§a"

---

# ğŸ¤– 4. Modelagem - O CoraÃ§Ã£o do Aprendizado

## ğŸ¯ **SeleÃ§Ã£o do Paradigma de Aprendizado**

### **Supervisionado** ğŸ¯ â†’ **Dados Rotulados**
```python
# Tarefas TÃ­picas
- ClassificaÃ§Ã£o: spam/ham, positivo/negativo
- RegressÃ£o: score de sentimento (0-10)
- NER: Reconhecimento de entidades
- POS Tagging: AnÃ¡lise gramatical
```

**Algoritmos:** SVM, Random Forest, BERT Fine-tuning

### **NÃ£o Supervisionado** ğŸ” â†’ **PadrÃµes Naturais**
```python
# Tarefas TÃ­picas  
- Clustering: agrupamento de tÃ³picos
- Topic Modeling: LDA, NMF
- Anomaly Detection: textos fora do padrÃ£o
```

**Algoritmos:** K-means, LDA, Autoencoders

### **Semi/Auto-Supervisionado** ğŸ§© â†’ **Escalabilidade**
```python
# EstratÃ©gias
- Self-training: modelo se auto-rotula
- Pre-training: BERT/GPT em corpus gigante
- Data augmentation: back-translation
```

**Vantagem:** Aproveita dados nÃ£o rotulados abundantes

## ğŸ—ï¸ **Arquiteturas Modernas**

### **Encoder-Decoder** (Seq2Seq)
```python
# AplicaÃ§Ãµes: TraduÃ§Ã£o, Resumo, Q&A
Input: "Como estÃ¡ o tempo?" 
â†’ Encoder: [context_vector]
â†’ Decoder: "EstÃ¡ ensolarado hoje."
```

### **Transformer-Based**
```python
# Mecanismo: Self-Attention + Feed Forward
- BERT: Encoder-only â†’ entendimento
- GPT: Decoder-only â†’ geraÃ§Ã£o  
- T5: Encoder-Decoder â†’ tarefas diversas
```

### **Fine-tuning EstratÃ©gico**
```python
# TÃ©cnicas AvanÃ§adas
1. Feature-based: Usar embeddings congelados
2. Full fine-tuning: Ajustar todos pesos
3. LoRA: Ajuste eficiente de grandes modelos
```

---

# ğŸš€ 5. Deploy - Do LaboratÃ³rio para o Mundo Real

## ğŸ¯ **Pipeline de ProduÃ§Ã£o**

### **1. Empacotamento do Modelo**
```python
# Formatos de SerializaÃ§Ã£o
- Pickle (sklearn) â†’ RÃ¡pido mas frÃ¡gil
- ONNX â†’ Cross-platform
- TensorFlow SavedModel â†’ Production-ready
- Hugging Face Pipeline â†’ NLP especializado
```

### **2. Infraestrutura de ServiÃ§o**
```python
# OpÃ§Ãµes de Deploy
â”œâ”€â”€ API REST (FastAPI/Flask)
â”œâ”€â”€ MicroserviÃ§os (Docker/K8s)  
â”œâ”€â”€ Serverless (AWS Lambda)
â””â”€â”€ Edge (Ort, TensorFlow Lite)
```

### **3. Monitoramento ContÃ­nuo** ğŸ“Š
| MÃ©trica | O que Monitorar | Alerta |
|---------|-----------------|--------|
| **Data Drift** | DistribuiÃ§Ã£o inputs muda | Retreinar modelo |
| **Concept Drift** | RelaÃ§Ã£o X-y muda | Reavaliar features |
| **Performance** | AcurÃ¡cia/F1 em produÃ§Ã£o | IntervenÃ§Ã£o imediata |
| **LatÃªncia** | Tempo resposta > SLA | Otimizar cÃ³digo |

### **4. CI/CD para ML** ğŸ”„
```yaml
# Pipeline Automatizado
1. Data Validation â†’ 2. Model Training â†’
2. Model Evaluation â†’ 4. A/B Testing â†’
3. Canary Deployment â†’ 6. Monitoring
```

### **5. ManutenÃ§Ã£o Proativa** ğŸ› ï¸
```python
# EstratÃ©gias de AtualizaÃ§Ã£o
- Retreino agendado: Semanal/Mensal
- Retreino sob demanda: Drift detection
- Shadow mode: Teste sem impacto
- A/B testing: ComparaÃ§Ã£o de versÃµes
```

## ğŸ¯ **PrincÃ­pio Fundamental**
**"Deploy nÃ£o Ã© o final do desenvolvimento, mas o inÃ­cio da observaÃ§Ã£o do comportamento do modelo no mundo real."**

Cada interaÃ§Ã£o do usuÃ¡rio Ã© um novo ponto de dados que pode (e deve) alimentar melhorias contÃ­nuas no sistema.

