# MÃ©todos de ClassificaÃ§Ã£o

> [!abstract] ## ðŸ“š DefiniÃ§Ã£o 
> **ClassificaÃ§Ã£o** Ã© um tipo de aprendizado supervisionado onde o objetivo Ã© prever rÃ³tulos discretos (categorias) com base em features de entrada.
> 
> Ela divide objetos com base em atributos conhecidos.

> [!example] **Exemplos de AplicaÃ§Ãµes**:
> - DiagnÃ³stico mÃ©dico (doente/saudÃ¡vel)
> - DetecÃ§Ã£o de spam (spam/nÃ£o-spam)
> - Reconhecimento de dÃ­gitos manuscritos (0-9)
> - Filtragem de spam  
> - DetecÃ§Ã£o de idioma  
> - Pesquisa de documentos semelhantes  
> - AnÃ¡lise de sentimentos  
> - Reconhecimento de caracteres e nÃºmeros

---
## ðŸ§  Principais Algoritmos

### 1. ðŸ“ˆ [[ML - RegressÃ£o Linear]]
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
```
- **PrÃ³s**: Simples, interpretÃ¡vel, bom para problemas lineares
- **Contras**: NÃ£o captura relaÃ§Ãµes nÃ£o-lineares complexas

### 2. ðŸŒ³ [[ML - Arvores de DecisÃ£o]]
```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth=3)
```
- **PrÃ³s**: InterpretÃ¡vel, lida bem com features categÃ³ricas
- **Contras**: TendÃªncia a overfitting

### 3. ðŸŒ²[[ML - Random Florest]]
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
```
- **PrÃ³s**: Reduz overfitting, boa performance geral
- **Contras**: Menos interpretÃ¡vel que Ã¡rvores simples

### 4. ðŸŽ¯ [[ML - SVM Maquina Vetorial de Suporte]]
```python
from sklearn.svm import SVC
model = SVC(kernel='rbf')
```
- **PrÃ³s**: Eficaz em espaÃ§os de alta dimensÃ£o
- **Contras**: Computacionalmente intensivo

### 5. ðŸ§  [[ML - Naive Bayes]]
```python
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
```
- **PrÃ³s**: Captura padrÃµes complexos
- **Contras**: Requer muitos dados e tuning
 

---

## ðŸ“Š MÃ©tricas de AvaliaÃ§Ã£o
| MÃ©trica               | FÃ³rmula                     | Quando Usar                 |
|-----------------------|----------------------------|----------------------------|
| **AcurÃ¡cia**          | (TP+TN)/(TP+TN+FP+FN)      | Classes balanceadas         |
| **PrecisÃ£o**          | TP/(TP+FP)                 | Custo de FP alto           |
| **Recall**            | TP/(TP+FN)                 | Custo de FN alto           |
| **F1-Score**          | 2*(Precision*Recall)/(Precision+Recall) | Balancear P/R |
| **Matriz de ConfusÃ£o**| -                          | AnÃ¡lise detalhada          |

---

## ðŸ› ï¸ Fluxo de Trabalho TÃ­pico
1. **PrÃ©-processamento**  
   - [[Limpeza de Dados]]
   - [[Feature Engineering]]
   - [[Balanceamento de Classes]] (SMOTE/Undersampling)

2. **Modelagem**  
   - [[SeleÃ§Ã£o de Algoritmo]]
   - [[ValidaÃ§Ã£o Cruzada]]

3. **AvaliaÃ§Ã£o**  
   - [[Teste com Dados NÃ£o Vistos]]
   - [[InterpretaÃ§Ã£o de Resultados]]

---
## ðŸ’¡ Dicas PrÃ¡ticas
- Comece sempre com **modelos simples** antes de ir para redes neurais
- Use **regularizaÃ§Ã£o** (L1/L2) para evitar overfitting
- Para classes desbalanceadas, priorize **F1-Score** em vez de acurÃ¡cia
- Visualize decisÃµes com **SHAP/LIME** para modelos complexos

> [!example] Exemplo PrÃ¡tico  
> ```python
> # Pipeline completo com Scikit-learn
> from sklearn.pipeline import make_pipeline
> from sklearn.preprocessing import StandardScaler
> 
> pipe = make_pipeline(
>     StandardScaler(),
>     RandomForestClassifier()
> )
> pipe.fit(X_train, y_train)
> print(pipe.score(X_test, y_test))
> ```
